# Gradient Algorithms Repository

This repository contains a collection of gradient algorithms implemented in various programming languages. Gradient algorithms are widely used in optimization and machine learning tasks for finding the minimum or maximum of a function.

## Algorithms Included

1. Gradient Descent
2. Stochastic Gradient Descent
3. Mini-Batch Gradient Descent
4. Conjugate Gradient Descent
5. Momentum-Based Gradient Descent
6. Nesterov Accelerated Gradient Descent
7. AdaGrad
8. RMSProp
9. Adam
10. AdaDelta

## Usage

Each algorithm is implemented in python as a separate file. The algorithms can be used by importing the specific file or module into your project.

### Python

To use the Python implementations, follow these steps:

1. Clone the repository:

```
git clone https://github.com/abomine/gradient.git
```

2. Import the desired algorithm in your Python script:

```python
from gradient import gradient_descent

# Use the gradient descent algorithm
gradient_descent()
```

3. Customize the algorithm parameters and integrate it into your code as needed.

## License

This repository is licensed under the [MIT License](LICENSE). Feel free to use the code for personal or commercial purposes.
